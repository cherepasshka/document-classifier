{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary includes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional functions\n",
    "\n",
    "garbage_words = stopwords.words('english') + stopwords.words('russian')\n",
    "\n",
    "\n",
    "def ClearText(text, garbage_words):\n",
    "    result = [word for word in text.lower().split() if word not in garbage_words]\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def TrainRandomForestClassifier(dataframe):\n",
    "    ngram_range = (1, 3)\n",
    "    max_features = 50000\n",
    "    n_estimators=100\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                tokenizer = None,\n",
    "                                preprocessor = None,\n",
    "                                stop_words = None, \n",
    "                                ngram_range = ngram_range,\n",
    "                                max_features = max_features\n",
    "                                )\n",
    "    X_train, Y_train = dataframe['X'], dataframe['target']\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_train = X_train.toarray()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    model = model.fit(X_train, Y_train)\n",
    "    return vectorizer, model\n",
    "\n",
    "\n",
    "def DepthFirstSearch(cur_folder, doc_types, file_names, doc_texts, name=''):\n",
    "    files = os.listdir()\n",
    "    cur_name = name[:]\n",
    "    if len(cur_name) != 0:\n",
    "        cur_name += '-'\n",
    "    cur_name += cur_folder\n",
    "    folders = [file for file in files if '.' not in file]\n",
    "    for folder in folders:\n",
    "        os.chdir(folder)\n",
    "        DepthFirstSearch(folder, doc_types, file_names, doc_texts, cur_name)\n",
    "        os.chdir('..')\n",
    "    if len(folders) == 0:\n",
    "        for file in files:\n",
    "            doc_types.append(cur_name)\n",
    "            file_names.append(file)\n",
    "            doc_texts.append('') # need to proceed the text layer of document \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file_name       doc_type doc_text\n",
      "0      aa.txt        a1-f_pg         \n",
      "1    absd.txt        a1-f_pg         \n",
      "2  fhghag.xml        a1-n_pg         \n",
      "3   kek12.txt  a2-f_pg-first         \n",
      "4   lol12.txt  a2-f_pg-first         \n",
      "5      sd.txt  a2-f_pg-page2         \n",
      "6  test54.txt        a2-n_pg         \n",
      "7  text42.txt        a2-n_pg         \n"
     ]
    }
   ],
   "source": [
    "# read input\n",
    "doc_types, file_names, doc_texts = [], [], []\n",
    "\n",
    "dataset_path = 'for training'\n",
    "os.chdir(dataset_path)\n",
    "DepthFirstSearch('', doc_types, file_names, doc_texts)\n",
    "os.chdir('..')\n",
    "df = pd.DataFrame({'file_name': file_names, 'doc_type': doc_types, 'doc_text' : doc_texts})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file_name       doc_type doc_text\n",
      "0      aa.txt        a1-f_pg         \n",
      "1    absd.txt        a1-f_pg         \n",
      "2  fhghag.xml        a1-n_pg         \n",
      "3   kek12.txt  a2-f_pg-first         \n",
      "4   lol12.txt  a2-f_pg-first         \n",
      "5      sd.txt  a2-f_pg-page2         \n",
      "6  test54.txt        a2-n_pg         \n",
      "7  text42.txt        a2-n_pg         \n"
     ]
    }
   ],
   "source": [
    "# modify input\n",
    "for i in range(len(df['doc_text'])):\n",
    "    df['doc_text'][i] = ClearText(df['doc_text'][i], garbage_words)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training (0.6 of dataframe) validation (0.2 of dataframe) and test (0.2 of dataframe)\n",
    "train, validate, test = np.split(df.sample(frac=1), [int(0.6 * len(df)), int(0.8 * len(df))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess accuracy of the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
