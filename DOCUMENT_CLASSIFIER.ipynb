{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary includes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional functions\n",
    "\n",
    "garbage_words = stopwords.words('english') + stopwords.words('russian')\n",
    "\n",
    "\n",
    "def ClearText(text, garbage_words):\n",
    "    result = [word for word in text.lower().split() if word not in garbage_words]\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def TrainRandomForestClassifier(dataframe):\n",
    "    ngram_range = (1, 3)\n",
    "    max_features = 50000\n",
    "    n_estimators = 300\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                tokenizer = None,\n",
    "                                preprocessor = None,\n",
    "                                stop_words = None, \n",
    "                                ngram_range = ngram_range,\n",
    "                                max_features = max_features\n",
    "                                )\n",
    "    X_train, Y_train = dataframe['X'], dataframe['target']\n",
    "    \n",
    "    \n",
    "    #print(X_train.shape)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    #print(X_train.shape)\n",
    "    #print(type(X_train))\n",
    "    # X_train = X_train.toarray()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    model = model.fit(X_train, Y_train)\n",
    "    return vectorizer, model\n",
    "\n",
    "\n",
    "def DepthFirstSearch(cur_folder, doc_types, file_names, doc_texts, name=''):\n",
    "    files = os.listdir()\n",
    "    cur_name = name[:]\n",
    "    if len(cur_name) != 0:\n",
    "        cur_name += '-'\n",
    "    cur_name += cur_folder\n",
    "    if 'text_layer' in files:\n",
    "        files.remove('text_layer')\n",
    "        \n",
    "    folders = [file for file in files if '.' not in file]\n",
    "    \n",
    "    for folder in folders:\n",
    "        os.chdir(folder)\n",
    "        DepthFirstSearch(folder, doc_types, file_names, doc_texts, cur_name)\n",
    "        os.chdir('..')\n",
    "    if len(folders) == 0:\n",
    "        for file in files:\n",
    "            #print(os.listdir())\n",
    "            text_layer = ''\n",
    "            if 'text_layer' in os.listdir():\n",
    "                try:\n",
    "                    with open('./text_layer/' + file.split('.')[0] + '.txt') as f:\n",
    "                        text_layer = f.read()\n",
    "                except:\n",
    "                    # print('file ', file, 'in', cur_name, 'not found')\n",
    "                    NOT_FOUND.append((file, cur_name))\n",
    "            if len(text_layer) > 0:\n",
    "                doc_texts.append(text_layer)\n",
    "                doc_types.append(cur_name)\n",
    "                file_names.append(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.ipynb_checkpoints', 'DOCUMENT_CLASSIFIER.ipynb', 'for training', 'models', 'outlook message extractor.ipynb', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "#os.chdir('..')\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input\n",
    "NOT_FOUND = []\n",
    "doc_types, file_names, doc_texts = [], [], []\n",
    "\n",
    "dataset_path = 'for training'\n",
    "os.chdir(dataset_path)\n",
    "DepthFirstSearch('', doc_types, file_names, doc_texts)\n",
    "os.chdir('..')\n",
    "df = pd.DataFrame({'file_name': file_names, 'doc_type': doc_types, 'doc_text' : doc_texts})\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JPEG', 'pdf', 'PDF', 'jpg', 'msg'}\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "ext = [x[0].split('.')[-1] for x in NOT_FOUND]\n",
    "print(set(ext))\n",
    "formated_not_found = ['file {} at {} not found'.format(x[0], x[1]) for x in NOT_FOUND if '' in x[0].split('.')[-1].lower()]\n",
    "print(len(formated_not_found))\n",
    "#print('\\n'.join(formated_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['doc_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify input\n",
    "\n",
    "for i in range(len(df['doc_text'])):\n",
    "    df['doc_text'][i] = ClearText(df['doc_text'][i], garbage_words)\n",
    "df = df.rename(columns={'doc_type' : 'target', 'doc_text' : 'X'})\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training (0.6 of dataframe) validation (0.2 of dataframe) and test (0.2 of dataframe)\n",
    "\n",
    "# train, validate, test = np.split(df.sample(frac=1), [int(0.6 * len(df)), int(0.8 * len(df))])\n",
    "train, test = np.split(df.sample(frac=1), [int(0.7 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "vectorizer, model = TrainRandomForestClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model to file\n",
    "if 'models' not in os.listdir():\n",
    "    os.mkdir('models')\n",
    "with open('./models/doc_classifyer-model', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('./models/doc_classifyer-vectorizer', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/doc_classifyer-model', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open('./models/doc_classifyer-vectorizer', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9012446997674737\n",
      "mean probability: 0.03225806451612903\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(vectorizer.transform(test['X']))\n",
    "probability = model.predict_proba(vectorizer.transform(test['X']))\n",
    "accuracy = np.mean(prediction == test['target'])\n",
    "print(accuracy)\n",
    "print('mean probability:', np.mean(probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              precision    recall  f1-score   support\n",
      "\n",
      "                                            ad-invoice-first       0.93      0.92      0.93      1299\n",
      "                                            ad-invoice-multi       0.92      0.75      0.82       147\n",
      "                                          ad-statement-first       0.98      0.99      0.99      1671\n",
      "                                          ad-statement-multi       0.98      0.91      0.94       216\n",
      "                                                ad-upd-first       0.89      0.95      0.92      1081\n",
      "                                                ad-upd-multi       0.92      0.82      0.86       223\n",
      "                                            ad-waybill-first       0.93      0.95      0.94       408\n",
      "                                            ad-waybill-multi       1.00      0.70      0.82        53\n",
      "                                                     rfq-AOG       0.88      0.60      0.71       292\n",
      "                                                rfq-CRITICAL       0.61      0.42      0.50       263\n",
      "                                                 rfq-Routine       0.81      0.95      0.87      1131\n",
      "                                                    rfq-Spam       1.00      0.96      0.98       377\n",
      "                                      sec-Изменение контроля       0.91      0.91      0.91        11\n",
      "                                            sec-Кросс-дефолт       0.28      0.25      0.26        20\n",
      "                      sec-Обозначение прав дочерних компаний       0.67      0.50      0.57         4\n",
      "                       sec-Оговорки о коллективных действиях       0.95      1.00      0.97        19\n",
      "                                sec-Ограничение деятельности       0.00      0.00      0.00         4\n",
      "                               sec-Ограничение задолженности       0.00      0.00      0.00         8\n",
      "             sec-Ограничение задолженности дочерних компаний       0.00      0.00      0.00         3\n",
      "                              sec-Ограничение по инвестициям       0.00      0.00      0.00         0\n",
      "sec-Ограничение по наслоению долговых обязательств по рангам       1.00      0.33      0.50         3\n",
      "                                 sec-Ограничение по платежам       1.00      1.00      1.00         3\n",
      "   sec-Ограничение по платежам в отношении дочерних компаний       1.00      0.40      0.57         5\n",
      "                    sec-Ограничение по предоставлению залога       0.58      0.95      0.72        19\n",
      "                          sec-Ограничение по продаже активов       0.50      0.40      0.44         5\n",
      "       sec-Ограничение по продаже активов с обратной арендой       0.00      0.00      0.00         5\n",
      "                                  sec-Ограничение по слиянию       1.00      0.57      0.73         7\n",
      "     sec-Ограничение по транзакциям с аффилированными лицами       0.67      0.67      0.67         6\n",
      "                                          sec-Случаи дефолта       0.29      0.32      0.30        19\n",
      "                sec-Условие приостановки действия ковенантов       0.67      0.67      0.67         3\n",
      "                                    sec-Финансовые ковенанты       0.14      0.17      0.15         6\n",
      "\n",
      "                                                    accuracy                           0.90      7311\n",
      "                                                   macro avg       0.66      0.58      0.60      7311\n",
      "                                                weighted avg       0.90      0.90      0.90      7311\n",
      "\n",
      "0.9012446997674737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otrm\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\otrm\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\otrm\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# print(confusion_matrix(test['target'], prediction))\n",
    "print(classification_report(test['target'], prediction))\n",
    "print(accuracy_score(test['target'], prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test set: 7311\n",
      "Statistics:\n",
      "Precision = 77.09%; Ratio of wrong answers = 20.43%; Confidence = 0.8\n"
     ]
    }
   ],
   "source": [
    "# assess accuracy of the model\n",
    "ds = test\n",
    "confidence_level = 0.8\n",
    "ds['predict'] = model.predict(vectorizer.transform(ds['X']))\n",
    "ds['confidence'] = np.transpose(np.amax(model.predict_proba(vectorizer.transform(ds['X'])), axis=1))\n",
    "\n",
    "\n",
    "precision = round(100 * len(ds[(ds['confidence'] >= confidence_level) & (ds['target'] == ds['predict'])]) / len(ds), 2)\n",
    "to_validate = round(100 * len(ds[(ds['confidence'] < confidence_level)]) / len(ds), 2)\n",
    "    \n",
    "print('Rows in test set: {}'.format(len(ds)))\n",
    "print('Statistics:')\n",
    "print('Precision = {}%; Ratio of wrong answers = {}%; Confidence = {}'.format(precision, to_validate, confidence_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
